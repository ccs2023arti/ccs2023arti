<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>index</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="weakcamid-artifact-evaluation---ccs-2023">WeakCamID artifact evaluation - CCS 2023</h1>
<h3 id="weakcamid--brief"><strong>WeakCamID</strong>  Brief</h3>
<p>Wireless security cameras may deter intruders. Accompanying the hardware, consumers may pay recurring monthly fees for recording videos to the cloud, or use the free tier offering motion alerts and sometimes live streams via the camera app. Many users may purchase the hardware without buying the subscription to save money, which inherently reduces their efficacy. We discover that the wireless traffic generated by a camera responding to stimulating motion may disclose whether or not video is being streamed. A malicious user such as a burglar may use such knowledge to target homes with a “weak camera” that does not upload video or turn on live view mode. In such cases, criminal activities would not be recorded though they are performed within the monitoring area of the camera. Accordingly, we describe a novel technique called WeakCamID that creates motion stimuli and sniffs resultant wireless traffic to infer the camera state. We perform a survey involving a total of 220 users, finding that all users think cameras have a consistent security guarantee regardless of the subscription status. Our discovery breaks such “common sense”. We implement <strong>WeakCamID</strong> in a mobile app and experiment with 11 popular wireless cameras to show that <strong>WeakCamID</strong> can identify weak cameras with a mean accuracy of around 95% and within less than 19 seconds.</p>
<p><a href="http://weakcamid.info/paper.pdf">Read our latest version of full paper</a>.</p>
<p>We are going to claim the <strong>Artifacts evaluated – Artifacts Functional</strong>, because our experiment environment and network environment are pre-designed and our phone app are currently experimental and still need some work.</p>
<h2 id="artifact-evaluation-sections">Artifact evaluation sections</h2>
<p>Here are the list of our artifact evaluation materials, and <strong>each item in this guide will lead to an indivinal section of processes we did for get the result</strong>:</p>
<h3 id="artifact-inventory-list---">Artifact inventory list –</h3>
<h5 id="below-are-the-artifacts-that-generate-the-corresponding-results.-we-select-high-relevant-experiment-result-from-our-paper-and-present-here-how-we-get-the-result.-we-will-show-the-detailed-data-collecting-methods-and-data-processing-preceduces-in-each-item-in-our-inventory.-each-item-in-this-guide-will-lead-to-an-indivinal-section-of-processes-we-did-for-get-the-result.-click-go-to-button-to-view-individual-artifact-page.">Below are the artifacts that generate the corresponding results. We select high relevant experiment result from our paper and present here how we get the result. We will show the detailed data collecting methods and data processing preceduces in each item in our inventory. <strong>Each item in this guide will lead to an indivinal section of processes we did for get the result</strong>. Click <code>Go to</code> button to view individual artifact page.</h5>
<hr>
<h5 id="camera-devices-connected-to-2.4-ghz-wifi-is-required-to-run-the-traffic-collection-for-each-of-the-task.-its-suggested-that-you-have-a-wide-open-room-with-empty-space-for-camera-deployment-should-have-at-least-two-camera-models-from-the-models-listed-on-table-1-like-ex.-alro-pro-4.--two-same-camera-for-each-camera-model-preferred-to-test.-one-camera-is-working-in-free-plan-no-cloud-recording-function-and-another-one-works-in-paid-subscription">Camera devices connected to 2.4 Ghz WiFi is required to run the traffic collection for each of the task. It’s suggested that you have a wide open room with empty space for camera deployment, should have at least two camera models from the models listed on Table 1 like (ex. Alro Pro 4).  Two same camera for each camera model preferred to test. One camera is working in free plan (no cloud recording function) and another one works in paid subscription</h5>
<h5 id="dataset-and-data-processing-guide.-go-to">1. Dataset and Data processing guide. <a href="https://weakcamid.info/dataprocessing.html">Go to</a></h5>
<ul>
<li>Download the raw wireless traffic <a href="https://weakcamid.info/rawdata.zip">dataset</a> captured from commercial off-the-shelf security cameras listed on Table 1.</li>
<li>Instruction for how to process the data at <a href="https://weakcamid.info/dataprocessing.html">Data Processing and Training Guide</a>, with a sample data stream.</li>
</ul>
<h5 id="comparison-of-success-rates-and-f1-scores-within-three-classifiers.-go-to">2. Comparison of success rates and F1 Scores within three classifiers. <a href="https://weakcamid.info/classcompare.html">Go to</a></h5>
<ul>
<li>Camera traffic identification via three different classifiers: DT, RF, SVM.</li>
<li>Successful rates (Figure 3).</li>
<li>F1 Scores (Figure 4).</li>
</ul>
<hr>
<h5 id="motion-duration-result.-go-to">3. Motion duration result. <a href="https://weakcamid.info/duration.html">Go to</a></h5>
<ul>
<li>Impact of motion duration (Figure 12)</li>
<li>F1 score vs. motion duration. (Figure 13)</li>
<li>Average Success Rates and F1 Scores. (Figure 14 and Figure 15)</li>
</ul>
<hr>
<h5 id="impact-of-movement-speed.-go-to">4. Impact of movement speed. <a href="https://weakcamid.info/newcamera.html">Go to</a></h5>
<ul>
<li>Accuracy for new cameras. (Figure 16)</li>
<li>Time spent for new cameras. (Figure 17)</li>
</ul>
<hr>
<h5 id="impact-of-previously-unknown-cameras.-go-to">5. Impact of previously unknown cameras. <a href="https://weakcamid.info/newcamera.html">Go to</a></h5>
<ul>
<li>Accuracy for new cameras. (Figure 18)</li>
<li>Time spent for new cameras. (Figure 19)</li>
</ul>
<h5 id="overall-inference-performance.-go-to">6. Overall inference performance. <a href="https://weakcamid.info/overall.html">Go to</a></h5>
<ul>
<li>Indoor success rates. (Figure 20)</li>
<li>Outdoor success rates. (Figure 21)</li>
<li>Overall confusion matrix.(Figure 22)</li>
<li>CDF of the detection time. (Figure 23)</li>
</ul>
<h5 id="general-analysis-on-the-distinctiveness-of-camera-traffic.-go-to">7. General analysis on the distinctiveness of camera traffic. <a href="https://weakcamid.info/userstudy.html">Go to</a></h5>
<ul>
<li>Data collection methods</li>
<li>Individual detection time for 11 users. (Figure 26)</li>
</ul>
<hr>
<h5 id="you-can-read-each-section-in-its-indivdual-page-and-if-you-press-home-button-located-on-both-the-top-and-bottom-of-page-then-you-can-come-back-to-artifact-evaluation-guide-home.">You can read each section in its indivdual page, and if you press <code>Home</code> button located on both the top and bottom of page then you can come back to artifact evaluation guide home.</h5>
<h2 id="cameras-we-use-in-experiment-table-1">Cameras we use in experiment (Table 1)</h2>

<table>
<thead>
<tr>
<th>ID</th>
<th>Camera Name</th>
<th>Cloud Recording (Unpaid)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Arlo Pro 3</td>
<td>No</td>
</tr>
<tr>
<td>2</td>
<td>Arlo Pro 4</td>
<td>No</td>
</tr>
<tr>
<td>3</td>
<td>Arlo Ultra 2</td>
<td>No</td>
</tr>
<tr>
<td>4</td>
<td>Blink XT2</td>
<td>No</td>
</tr>
<tr>
<td>5</td>
<td>Blink Outdoor</td>
<td>No</td>
</tr>
<tr>
<td>6</td>
<td>Ring Stick Up Cam</td>
<td>No</td>
</tr>
<tr>
<td>7</td>
<td>Ring Spotlight</td>
<td>No</td>
</tr>
<tr>
<td>8</td>
<td>Reolink Argus 2</td>
<td>No</td>
</tr>
<tr>
<td>9</td>
<td>SimpliSafe Cam</td>
<td>No</td>
</tr>
<tr>
<td>10</td>
<td>Wyze Battery Cam Pro</td>
<td>No</td>
</tr>
<tr>
<td>11</td>
<td>Wyze Cam Outdoor v2</td>
<td>No</td>
</tr>
</tbody>
</table><h2 id="software-prerequisites">Software prerequisites</h2>
<p>The following software/modules needed to get our result:</p>
<ul>
<li>Python 3.9
<ul>
<li>Scikit-learn 0.24</li>
<li>Numpy 1.13.3</li>
<li>Scipy 0.19.1</li>
<li>matplotlib 2.1.1</li>
<li>Pandas</li>
<li>SQLite 3</li>
</ul>
</li>
<li>Matlab R2023_a
<ul>
<li>Machine learning toolkit</li>
<li>Deep Learning toolkit</li>
</ul>
</li>
<li>Kali Nethunter 2023</li>
<li>QCACLD for Qualcomm SoC devices</li>
<li>For rooted smartphone’s monitor mode, BCM firmware, Nexmon firmware for BCM4339 and BCM4358</li>
<li>Android Studio 2023 with NDK support libraries (MacOS preferred for cross compilation)</li>
<li>Microsoft Excel 2023</li>
</ul>
<h4 id="beginner-101-for-traffic-scanning---collecting-camera-traffic-data-with-airodump">Beginner 101 for traffic scanning - Collecting Camera Traffic Data with airodump</h4>
<p>In order to know why camera’s traffic will not be the same under different working mode. We have a small quick playground for you to test the wireless sniffing. This quick guide explains how to collect wireless traffic data from security cameras in 4 different modes:</p>
<ul>
<li>Paid - Normal</li>
<li>Paid - Live View</li>
<li>Unpaid - Normal</li>
<li>Unpaid - Live View</li>
</ul>
<p>Using the airodump tool from the aircrack-ng suite. You need a WiFi adapter that supports monitor mode.</p>
<h2 id="requirements">Requirements</h2>
<ul>
<li>Wireless security camera(s)</li>
<li>WiFi adapter in monitor mode</li>
<li>airodump-ng</li>
</ul>
<h2 id="steps">Steps</h2>
<ol>
<li>
<p>Put WiFi adapter into monitor mode</p>
<pre class=" language-bash"><code class="prism  language-bash">airmon-ng start wlan0
</code></pre>
</li>
<li>
<p>Find channel of target camera from the list, match the OUI</p>
<pre class=" language-bash"><code class="prism  language-bash">airodump-ng wlan0mon
</code></pre>
<p>Note the channel of the target camera’s BSSID.</p>
</li>
<li>
<p>Capture on channel</p>
<pre class=" language-bash"><code class="prism  language-bash">airodump-ng -c 6 --bssid 00:11:22:33:44:55 -w camera wlan0mon
</code></pre>
<p>Replace 6 with camera channel and 00:11:22:33:44:55 with camera BSSID.</p>
</li>
<li>
<p>Generate motion to stimulate camera</p>
<ul>
<li>Paid - Normal: Let camera record normally</li>
<li>Paid - Live View: Open live view on camera</li>
<li>Unpaid - Normal: Generate motion, no live view</li>
<li>Unpaid - Live View: Generate motion, open live view</li>
</ul>
</li>
<li>
<p>Repeat step 4 in each mode</p>
</li>
<li>
<p>Press Ctrl+C to stop capture</p>
</li>
</ol>
<p>Now you can analyze these capture files to extract traffic patterns for each camera mode and you should be ready to get start with traffic capture.</p>
<p>The following <a href="http://weakcamid.info/app.html">Android APP</a> is used to do the experiment and getting the localization result.</p>
<p>You can send us any email if you meet problems when running our code or getting any result, Yan He (<a href="mailto:heyan@ou.edu">heyan@ou.edu</a>) or Fang Song (<a href="mailto:songf@ou.edu">songf@ou.edu</a>).</p>
<h2 id="examples-when-running-our-code">Examples when running our code</h2>
<p>When you entering the indivdual artifact evaluation section page. You will see what kind of tool(s) we use for getting our result. Generally, if we are using python to do the data visualization then here is the how to get you started:</p>
<pre class=" language-python"><code class="prism  language-python">python3 <span class="token punctuation">[</span>section name<span class="token punctuation">]</span><span class="token punctuation">.</span>py 
</code></pre>
<p>If you notice that we are using mathlab to show our final result and you want to  find <code>Matlab</code>  codes we shown on our paper with same result, read <code>Code</code> part which should be on the bottom of each section page.</p>
<pre><code>1. Copy the Matlab code shown on page
2. Create *.m file
3. Run the *.m file on Matlab
</code></pre>
<p>If the result is getting by Microsoft Excel, just simplely download the *.xls file shown on the individual page.</p>
</div>
</body>

</html>
